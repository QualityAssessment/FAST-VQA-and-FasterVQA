{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91da64be-faeb-43e6-a08b-61106253d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "from fastvqa.apis import deep_end_to_end_vqa\n",
    "from fastvqa.models import BaseEvaluator, VQABackbone\n",
    "from fastvqa.datasets import VQAInferenceDataset, get_fragments, SampleFrames\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from scipy.stats.stats import kendalltau as kendallr\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "## choose the device you would like to run on\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adee193d-ee6a-4254-a0f9-fd997999fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, False]\n",
      "0.36467987298965454\n"
     ]
    }
   ],
   "source": [
    "model = fastvqa.deep_end_to_end_vqa()\n",
    "\n",
    "video = torch.randn((3,96,224,224))\n",
    "score = model(video)\n",
    "print(score['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd7a5c-a34c-498a-9fc7-f42bd464c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 10 random split experiments,\n",
      "        the mean SROCC is 0.8864 (0.0251), median 0.8906\n",
      "        the mean PLCC  is 0.8982 (0.0125), median 0.8992\n",
      "        the mean KROCC is 0.7167 (0.0281), median 0.7234\n",
      "        the mean RMSE  is 9.1804 (0.7355), median 9.1566\n"
     ]
    }
   ],
   "source": [
    "## Calculate the finetune accuracy on small datasets\n",
    "\n",
    "def get_finetune_results(results):\n",
    "    if 'results' in results:\n",
    "        results = results['results']\n",
    "    srccs = np.array([r[0] for r in results])\n",
    "    plccs = np.array([r[1] for r in results])\n",
    "    krccs = np.array([r[2] for r in results])\n",
    "    rmse = np.array([r[3] for r in results])\n",
    "    ms, ss, mds = np.mean(srccs), np.std(srccs), np.median(srccs)\n",
    "    mp, sp, mdp = np.mean(plccs), np.std(plccs), np.median(plccs)\n",
    "    mk, sk, mdk = np.mean(krccs), np.std(krccs), np.median(krccs)\n",
    "    mr, sr, mdr = np.mean(rmse), np.std(rmse), np.median(rmse)\n",
    "    print(f'''In {len(results)} random split experiments,\n",
    "        the mean SROCC is {ms:.4f} ({ss:.4f}), median {mds:.4f}\n",
    "        the mean PLCC  is {mp:.4f} ({sp:.4f}), median {mdp:.4f}\n",
    "        the mean KROCC is {mk:.4f} ({sk:.4f}), median {mdk:.4f}\n",
    "        the mean RMSE  is {mr:.4f} ({sr:.4f}), median {mdr:.4f}''')\n",
    "\n",
    "finetune_results = torch.load('results/results_finetune_cvd2014_s32*32_ens1from_ar.pkl')\n",
    "get_finetune_results(finetune_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886390d-2959-4e64-9740-44f50ba0d0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57d53ce-9159-4ee5-aa35-0c0439a8c231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## defining model and loading checkpoint\n",
    "\n",
    "model = BaseEvaluator().to(device)\n",
    "fsize = 32\n",
    "load_path = f'pretrained_weights/all_aligned_fragments_{fsize}.pth'\n",
    "state_dict = torch.load(load_path, map_location='cpu')\n",
    "\n",
    "if 'state_dict' in state_dict:\n",
    "    state_dict = state_dict['state_dict']\n",
    "    from collections import OrderedDict\n",
    "    i_state_dict = OrderedDict()\n",
    "    for key in state_dict.keys():\n",
    "        if 'cls' in key:\n",
    "            tkey = key.replace('cls', 'vqa')\n",
    "            i_state_dict[tkey] = state_dict[key]\n",
    "        else:\n",
    "            i_state_dict[key] = state_dict[key]\n",
    "\n",
    "model.load_state_dict(i_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e0ffe3-8cf6-4ceb-b1e5-3a9233949fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['backbone.patch_embed.proj.weight', 'backbone.patch_embed.proj.bias', 'backbone.patch_embed.norm.weight', 'backbone.patch_embed.norm.bias', 'backbone.layers.0.blocks.0.norm1.weight', 'backbone.layers.0.blocks.0.norm1.bias', 'backbone.layers.0.blocks.0.attn.relative_position_bias_table', 'backbone.layers.0.blocks.0.attn.relative_position_index', 'backbone.layers.0.blocks.0.attn.qkv.weight', 'backbone.layers.0.blocks.0.attn.qkv.bias', 'backbone.layers.0.blocks.0.attn.proj.weight', 'backbone.layers.0.blocks.0.attn.proj.bias', 'backbone.layers.0.blocks.0.norm2.weight', 'backbone.layers.0.blocks.0.norm2.bias', 'backbone.layers.0.blocks.0.mlp.fc1.weight', 'backbone.layers.0.blocks.0.mlp.fc1.bias', 'backbone.layers.0.blocks.0.mlp.fc2.weight', 'backbone.layers.0.blocks.0.mlp.fc2.bias', 'backbone.layers.0.blocks.1.norm1.weight', 'backbone.layers.0.blocks.1.norm1.bias', 'backbone.layers.0.blocks.1.attn.relative_position_bias_table', 'backbone.layers.0.blocks.1.attn.relative_position_index', 'backbone.layers.0.blocks.1.attn.qkv.weight', 'backbone.layers.0.blocks.1.attn.qkv.bias', 'backbone.layers.0.blocks.1.attn.proj.weight', 'backbone.layers.0.blocks.1.attn.proj.bias', 'backbone.layers.0.blocks.1.norm2.weight', 'backbone.layers.0.blocks.1.norm2.bias', 'backbone.layers.0.blocks.1.mlp.fc1.weight', 'backbone.layers.0.blocks.1.mlp.fc1.bias', 'backbone.layers.0.blocks.1.mlp.fc2.weight', 'backbone.layers.0.blocks.1.mlp.fc2.bias', 'backbone.layers.0.downsample.reduction.weight', 'backbone.layers.0.downsample.norm.weight', 'backbone.layers.0.downsample.norm.bias', 'backbone.layers.1.blocks.0.norm1.weight', 'backbone.layers.1.blocks.0.norm1.bias', 'backbone.layers.1.blocks.0.attn.relative_position_bias_table', 'backbone.layers.1.blocks.0.attn.relative_position_index', 'backbone.layers.1.blocks.0.attn.qkv.weight', 'backbone.layers.1.blocks.0.attn.qkv.bias', 'backbone.layers.1.blocks.0.attn.proj.weight', 'backbone.layers.1.blocks.0.attn.proj.bias', 'backbone.layers.1.blocks.0.norm2.weight', 'backbone.layers.1.blocks.0.norm2.bias', 'backbone.layers.1.blocks.0.mlp.fc1.weight', 'backbone.layers.1.blocks.0.mlp.fc1.bias', 'backbone.layers.1.blocks.0.mlp.fc2.weight', 'backbone.layers.1.blocks.0.mlp.fc2.bias', 'backbone.layers.1.blocks.1.norm1.weight', 'backbone.layers.1.blocks.1.norm1.bias', 'backbone.layers.1.blocks.1.attn.relative_position_bias_table', 'backbone.layers.1.blocks.1.attn.relative_position_index', 'backbone.layers.1.blocks.1.attn.qkv.weight', 'backbone.layers.1.blocks.1.attn.qkv.bias', 'backbone.layers.1.blocks.1.attn.proj.weight', 'backbone.layers.1.blocks.1.attn.proj.bias', 'backbone.layers.1.blocks.1.norm2.weight', 'backbone.layers.1.blocks.1.norm2.bias', 'backbone.layers.1.blocks.1.mlp.fc1.weight', 'backbone.layers.1.blocks.1.mlp.fc1.bias', 'backbone.layers.1.blocks.1.mlp.fc2.weight', 'backbone.layers.1.blocks.1.mlp.fc2.bias', 'backbone.layers.1.downsample.reduction.weight', 'backbone.layers.1.downsample.norm.weight', 'backbone.layers.1.downsample.norm.bias', 'backbone.layers.2.blocks.0.norm1.weight', 'backbone.layers.2.blocks.0.norm1.bias', 'backbone.layers.2.blocks.0.attn.relative_position_bias_table', 'backbone.layers.2.blocks.0.attn.relative_position_index', 'backbone.layers.2.blocks.0.attn.qkv.weight', 'backbone.layers.2.blocks.0.attn.qkv.bias', 'backbone.layers.2.blocks.0.attn.proj.weight', 'backbone.layers.2.blocks.0.attn.proj.bias', 'backbone.layers.2.blocks.0.norm2.weight', 'backbone.layers.2.blocks.0.norm2.bias', 'backbone.layers.2.blocks.0.mlp.fc1.weight', 'backbone.layers.2.blocks.0.mlp.fc1.bias', 'backbone.layers.2.blocks.0.mlp.fc2.weight', 'backbone.layers.2.blocks.0.mlp.fc2.bias', 'backbone.layers.2.blocks.1.norm1.weight', 'backbone.layers.2.blocks.1.norm1.bias', 'backbone.layers.2.blocks.1.attn.relative_position_bias_table', 'backbone.layers.2.blocks.1.attn.relative_position_index', 'backbone.layers.2.blocks.1.attn.qkv.weight', 'backbone.layers.2.blocks.1.attn.qkv.bias', 'backbone.layers.2.blocks.1.attn.proj.weight', 'backbone.layers.2.blocks.1.attn.proj.bias', 'backbone.layers.2.blocks.1.norm2.weight', 'backbone.layers.2.blocks.1.norm2.bias', 'backbone.layers.2.blocks.1.mlp.fc1.weight', 'backbone.layers.2.blocks.1.mlp.fc1.bias', 'backbone.layers.2.blocks.1.mlp.fc2.weight', 'backbone.layers.2.blocks.1.mlp.fc2.bias', 'backbone.layers.2.blocks.2.norm1.weight', 'backbone.layers.2.blocks.2.norm1.bias', 'backbone.layers.2.blocks.2.attn.relative_position_bias_table', 'backbone.layers.2.blocks.2.attn.relative_position_index', 'backbone.layers.2.blocks.2.attn.qkv.weight', 'backbone.layers.2.blocks.2.attn.qkv.bias', 'backbone.layers.2.blocks.2.attn.proj.weight', 'backbone.layers.2.blocks.2.attn.proj.bias', 'backbone.layers.2.blocks.2.norm2.weight', 'backbone.layers.2.blocks.2.norm2.bias', 'backbone.layers.2.blocks.2.mlp.fc1.weight', 'backbone.layers.2.blocks.2.mlp.fc1.bias', 'backbone.layers.2.blocks.2.mlp.fc2.weight', 'backbone.layers.2.blocks.2.mlp.fc2.bias', 'backbone.layers.2.blocks.3.norm1.weight', 'backbone.layers.2.blocks.3.norm1.bias', 'backbone.layers.2.blocks.3.attn.relative_position_bias_table', 'backbone.layers.2.blocks.3.attn.relative_position_index', 'backbone.layers.2.blocks.3.attn.qkv.weight', 'backbone.layers.2.blocks.3.attn.qkv.bias', 'backbone.layers.2.blocks.3.attn.proj.weight', 'backbone.layers.2.blocks.3.attn.proj.bias', 'backbone.layers.2.blocks.3.norm2.weight', 'backbone.layers.2.blocks.3.norm2.bias', 'backbone.layers.2.blocks.3.mlp.fc1.weight', 'backbone.layers.2.blocks.3.mlp.fc1.bias', 'backbone.layers.2.blocks.3.mlp.fc2.weight', 'backbone.layers.2.blocks.3.mlp.fc2.bias', 'backbone.layers.2.blocks.4.norm1.weight', 'backbone.layers.2.blocks.4.norm1.bias', 'backbone.layers.2.blocks.4.attn.relative_position_bias_table', 'backbone.layers.2.blocks.4.attn.relative_position_index', 'backbone.layers.2.blocks.4.attn.qkv.weight', 'backbone.layers.2.blocks.4.attn.qkv.bias', 'backbone.layers.2.blocks.4.attn.proj.weight', 'backbone.layers.2.blocks.4.attn.proj.bias', 'backbone.layers.2.blocks.4.norm2.weight', 'backbone.layers.2.blocks.4.norm2.bias', 'backbone.layers.2.blocks.4.mlp.fc1.weight', 'backbone.layers.2.blocks.4.mlp.fc1.bias', 'backbone.layers.2.blocks.4.mlp.fc2.weight', 'backbone.layers.2.blocks.4.mlp.fc2.bias', 'backbone.layers.2.blocks.5.norm1.weight', 'backbone.layers.2.blocks.5.norm1.bias', 'backbone.layers.2.blocks.5.attn.relative_position_bias_table', 'backbone.layers.2.blocks.5.attn.relative_position_index', 'backbone.layers.2.blocks.5.attn.qkv.weight', 'backbone.layers.2.blocks.5.attn.qkv.bias', 'backbone.layers.2.blocks.5.attn.proj.weight', 'backbone.layers.2.blocks.5.attn.proj.bias', 'backbone.layers.2.blocks.5.norm2.weight', 'backbone.layers.2.blocks.5.norm2.bias', 'backbone.layers.2.blocks.5.mlp.fc1.weight', 'backbone.layers.2.blocks.5.mlp.fc1.bias', 'backbone.layers.2.blocks.5.mlp.fc2.weight', 'backbone.layers.2.blocks.5.mlp.fc2.bias', 'backbone.layers.2.downsample.reduction.weight', 'backbone.layers.2.downsample.norm.weight', 'backbone.layers.2.downsample.norm.bias', 'backbone.layers.3.blocks.0.norm1.weight', 'backbone.layers.3.blocks.0.norm1.bias', 'backbone.layers.3.blocks.0.attn.relative_position_bias_table', 'backbone.layers.3.blocks.0.attn.relative_position_index', 'backbone.layers.3.blocks.0.attn.qkv.weight', 'backbone.layers.3.blocks.0.attn.qkv.bias', 'backbone.layers.3.blocks.0.attn.proj.weight', 'backbone.layers.3.blocks.0.attn.proj.bias', 'backbone.layers.3.blocks.0.norm2.weight', 'backbone.layers.3.blocks.0.norm2.bias', 'backbone.layers.3.blocks.0.mlp.fc1.weight', 'backbone.layers.3.blocks.0.mlp.fc1.bias', 'backbone.layers.3.blocks.0.mlp.fc2.weight', 'backbone.layers.3.blocks.0.mlp.fc2.bias', 'backbone.layers.3.blocks.1.norm1.weight', 'backbone.layers.3.blocks.1.norm1.bias', 'backbone.layers.3.blocks.1.attn.relative_position_bias_table', 'backbone.layers.3.blocks.1.attn.relative_position_index', 'backbone.layers.3.blocks.1.attn.qkv.weight', 'backbone.layers.3.blocks.1.attn.qkv.bias', 'backbone.layers.3.blocks.1.attn.proj.weight', 'backbone.layers.3.blocks.1.attn.proj.bias', 'backbone.layers.3.blocks.1.norm2.weight', 'backbone.layers.3.blocks.1.norm2.bias', 'backbone.layers.3.blocks.1.mlp.fc1.weight', 'backbone.layers.3.blocks.1.mlp.fc1.bias', 'backbone.layers.3.blocks.1.mlp.fc2.weight', 'backbone.layers.3.blocks.1.mlp.fc2.bias', 'backbone.norm.weight', 'backbone.norm.bias', 'vqa_head.fc_hid.weight', 'vqa_head.fc_hid.bias', 'vqa_head.fc_last.weight', 'vqa_head.fc_last.bias'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_position_bias_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ad0fe-d8c7-4634-a82e-b4964bf3733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting datasets (if you want to load from existing VQA datasets)\n",
    "\n",
    "dataset_name = 'KoNViD'\n",
    "dataset_path = f'/mnt/lustre/hnwu/datasets/{dataset_name}'\n",
    "\n",
    "inference_set = VQAInferenceDataset(f'{dataset_path}/labels.txt', dataset_path, )\n",
    "                                    #fragments = 224 // fsize, fsize = fsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e04e1-7891-449c-8682-f61f72810b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the model with examplar fragment video\n",
    "## with ultra...fast performance\n",
    "\n",
    "## for example from the dataset\n",
    "\n",
    "q = random.randrange(len(inference_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83a188-15e8-40b4-a620-e2b167ed03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inference_set[q]\n",
    "\n",
    "st = time()\n",
    "\n",
    "\n",
    "\n",
    "vfrag = data['video'].to(device)\n",
    "\n",
    "## or, directly get from your input videos as follows\n",
    "## where 'video' is a torch Tensor\n",
    "\n",
    "## from datasets import temporal_sampling (not implemented yet)\n",
    "\n",
    "# data = temporal_sampling(video, 32, 2, 4)\n",
    "\n",
    "# vfrag = get_fragments(data['video']).to(device)\n",
    "\n",
    "demo_result = model(vfrag)\n",
    "print(demo_result.shape)\n",
    "demo_result = demo_result.reshape((-1,) + demo_result.shape[-2:])\n",
    "score = torch.mean(demo_result)\n",
    "end = time()\n",
    "\n",
    "print(f'The quality of the video is {score.item()}, consuming time {end-st:.4f}s.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e742acd-383d-472b-b8a6-307552fc020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(pr, gt=None):\n",
    "    if gt is None:\n",
    "        pr = ((pr - np.mean(pr)) / np.std(pr))\n",
    "    else:\n",
    "        pr = ((pr - np.mean(pr)) / np.std(pr)) * np.std(gt) + np.mean(gt)\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93aa92d-5ac5-4e98-b920-41d9bcf9c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "## see the spatial-temporal quality localization for a reference\n",
    "def init_demo_reader(path, i):\n",
    "    from decord import VideoReader, cpu\n",
    "    video_names = [ele.split(',')[0] for ele in open(f'{path}/labels.txt').readlines()]\n",
    "    frame_reader = VideoReader(f'{path}/{video_names[i]}', ctx=cpu(0))\n",
    "    return frame_reader\n",
    "\n",
    "frame_reader = init_demo_reader(dataset_path, q)\n",
    "video_names = [ele.split(',')[0] for ele in open(f'{dataset_path}/labels.txt').readlines()]\n",
    "\n",
    "r_index = random.randrange(len(data['frame_inds']))\n",
    "frame_index = data['frame_inds'][r_index]\n",
    "frame = frame_reader[frame_index]\n",
    "frame_quality_map = demo_result[r_index // 2]\n",
    "fragment = (vfrag.permute(0,2,3,4,1).reshape((128,) + vfrag.shape[-2:] + (3,)).cpu() * inference_set.std + inference_set.mean).numpy()[r_index]\n",
    "\n",
    "frame_quality_map = frame_quality_map.cpu().numpy()\n",
    "qlt = cv2.resize(rescale(frame_quality_map), (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "plt.figure(dpi=450)\n",
    "plt.subplot(221)\n",
    "plt.imshow(frame)\n",
    "plt.subplot(222)\n",
    "plt.imshow(frame / 255. - np.stack((qlt,) + (np.zeros_like(qlt),)*2, -1) / 2.)\n",
    "\n",
    "fqlt = cv2.resize(frame_quality_map, (fragment.shape[1], fragment.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "plt.figure(dpi=300)\n",
    "plt.subplot(221)\n",
    "plt.imshow(fragment / 255.)\n",
    "plt.subplot(222)\n",
    "plt.imshow(fqlt, cmap='gray') #fragment / 255. - np.stack((fqlt,) + (np.zeros_like(fqlt),)*2, -1) / 2.)\n",
    "#plt.savefig(f'demos/demo_{video_names[q].split(\"/\")[-1]}.png')\n",
    "print(frame_quality_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7853b-d2c2-456f-a43d-623f7fb41a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## run inference for a whole testing database\n",
    "## note that the Jupyter program might be relatively slower than running directly with './inference_dataset.py'\n",
    "\n",
    "inference_loader = torch.utils.data.DataLoader(inference_set, batch_size=1, num_workers=4)\n",
    "results = []\n",
    "\n",
    "for i, data in tqdm(enumerate(inference_loader)):\n",
    "    result = dict()\n",
    "    vqfrag = data['video'].to(device).squeeze(0)\n",
    "    with torch.no_grad():\n",
    "        result['pr_labels'] = model(vfrag).cpu().numpy()\n",
    "    result['gt_label'] = data['gt_label'].item()\n",
    "    result['frame_inds'] = data['frame_inds']\n",
    "                                                                                                                                                                                                                                                                                        del data\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57db71-e42b-4cb7-8a9f-22859f34d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating several accuracies indices\n",
    "\n",
    "gt_labels = [r['gt_label'] for r in results]\n",
    "pr_labels = [np.mean(r['pr_labels'][:]) for r in results]\n",
    "opr_labels = pr_labels\n",
    "pr_labels = rescale(pr_labels, gt_labels)\n",
    "\n",
    "srocc = spearmanr(gt_labels, pr_labels)[0]\n",
    "plcc = pearsonr(gt_labels, pr_labels)[0]\n",
    "krocc = kendallr(gt_labels, pr_labels)[0]\n",
    "rmse = np.sqrt(((gt_labels - pr_labels) ** 2).mean())\n",
    "\n",
    "print(f'For dataset {dataset_name} we inference, the accuracy of the model is as follows:\\n  SROCC: {srocc:.4f}\\n  PLCC:  {plcc:.4f}\\n  KROCC: {krocc:.4f}\\n  RMSE:  {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36bf51-0fa1-4bd4-a524-9e327fb39fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ten_133 - np.mean(opr_labels)) / np.std(opr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2878d4-dc68-43ef-be4a-c13443a7bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stableness of Fragments\n",
    "\n",
    "ten_102 = [-0.1089, -0.1156, -0.1259, -0.1196, -0.1227, -0.1156, -0.1100, -0.1113, -0.1181, -0.1126]\n",
    "print('Video No.102', np.mean(ten_102), np.std(ten_102))\n",
    "ten_133 = [-0.0748, -0.0671, -0.0784, -0.0530, -0.0521, -0.0653, -0.0456, -0.0777, -0.0378, -0.0584]\n",
    "print('Video No.133', np.mean(ten_133), np.std(ten_133))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd759b-6400-4d88-b199-f878e985c5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926c587-2cf6-446d-bbd5-b0bd972a9c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_Debug",
   "language": "python",
   "name": "pt1.8v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
