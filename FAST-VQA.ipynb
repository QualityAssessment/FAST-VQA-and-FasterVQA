{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91da64be-faeb-43e6-a08b-61106253d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "import fastvqa\n",
    "from fastvqa.models import BaseEvaluator\n",
    "from fastvqa.datasets import VQAInferenceDataset, get_fragments, SampleFrames\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from scipy.stats.stats import kendalltau as kendallr\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "## choose the device you would like to run on\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ac58e4-9495-4291-9662-f1c8e0d46397",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demo on how to run FAST-VQA in script \n",
    "model = fastvqa.deep_end_to_end_vqa(True, 'pretrained_weights/all_aligned_fragments_32.pth', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee193d-ee6a-4254-a0f9-fd997999fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/hnwu/anaconda3/envs/pt1.8v/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "video = torch.randn((3,96,224,224)).to(device)\n",
    "score = model(video)\n",
    "print(score['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd7a5c-a34c-498a-9fc7-f42bd464c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the finetune accuracy on small datasets\n",
    "\n",
    "def get_finetune_results(results):\n",
    "    if 'results' in results:\n",
    "        results = results['results']\n",
    "    srccs = np.array([r[0] for r in results])\n",
    "    plccs = np.array([r[1] for r in results])\n",
    "    krccs = np.array([r[2] for r in results])\n",
    "    ms, ss, mds = np.mean(srccs), np.std(srccs), np.median(srccs)\n",
    "    mp, sp, mdp = np.mean(plccs), np.std(plccs), np.median(plccs)\n",
    "    mk, sk, mdk = np.mean(krccs), np.std(krccs), np.median(krccs)\n",
    "    print(f'''In {len(results)} random split experiments,\n",
    "        the mean SROCC is {ms:.4f} ({ss:.4f}), median {mds:.4f}\n",
    "        the mean PLCC  is {mp:.4f} ({sp:.4f}), median {mdp:.4f}\n",
    "        the mean KROCC is {mk:.4f} ({sk:.4f}), median {mdk:.4f}''')\n",
    "\n",
    "finetune_results = torch.load('results/results_finetune_konvid_s32*32_ens1.pkl')\n",
    "get_finetune_results(finetune_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886390d-2959-4e64-9740-44f50ba0d0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d53ce-9159-4ee5-aa35-0c0439a8c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining model and loading checkpoint\n",
    "\n",
    "model = BaseEvaluator().to(device)\n",
    "fsize = 32\n",
    "load_path = f'pretrained_weights/all_aligned_fragments_{fsize}.pth'\n",
    "state_dict = torch.load(load_path, map_location='cpu')\n",
    "\n",
    "if 'state_dict' in state_dict:\n",
    "    state_dict = state_dict['state_dict']\n",
    "    from collections import OrderedDict\n",
    "    i_state_dict = OrderedDict()\n",
    "    for key in state_dict.keys():\n",
    "        if 'cls' in key:\n",
    "            tkey = key.replace('cls', 'vqa')\n",
    "            i_state_dict[tkey] = state_dict[key]\n",
    "        else:\n",
    "            i_state_dict[key] = state_dict[key]\n",
    "\n",
    "model.load_state_dict(i_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ad0fe-d8c7-4634-a82e-b4964bf3733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting datasets (if you want to load from existing VQA datasets)\n",
    "\n",
    "dataset_name = 'KoNViD'\n",
    "dataset_path = f'/mnt/lustre/hnwu/datasets/{dataset_name}'\n",
    "\n",
    "inference_set = VQAInferenceDataset(f'{dataset_path}/labels.txt', dataset_path, )\n",
    "                                    #fragments = 224 // fsize, fsize = fsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e04e1-7891-449c-8682-f61f72810b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the model with examplar fragment video\n",
    "## with ultra...fast performance\n",
    "\n",
    "## for example from the dataset\n",
    "\n",
    "q = random.randrange(len(inference_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83a188-15e8-40b4-a620-e2b167ed03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inference_set[q]\n",
    "\n",
    "st = time()\n",
    "\n",
    "\n",
    "\n",
    "vfrag = data['video'].to(device)\n",
    "\n",
    "## or, directly get from your input videos as follows\n",
    "## where 'video' is a torch Tensor\n",
    "\n",
    "## from datasets import temporal_sampling (not implemented yet)\n",
    "\n",
    "# data = temporal_sampling(video, 32, 2, 4)\n",
    "\n",
    "# vfrag = get_fragments(data['video']).to(device)\n",
    "\n",
    "demo_result = model(vfrag)\n",
    "print(demo_result.shape)\n",
    "demo_result = demo_result.reshape((-1,) + demo_result.shape[-2:])\n",
    "score = torch.mean(demo_result)\n",
    "end = time()\n",
    "\n",
    "print(f'The quality of the video is {score.item()}, consuming time {end-st:.4f}s.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e742acd-383d-472b-b8a6-307552fc020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(pr, gt=None):\n",
    "    if gt is None:\n",
    "        pr = ((pr - np.mean(pr)) / np.std(pr))\n",
    "    else:\n",
    "        pr = ((pr - np.mean(pr)) / np.std(pr)) * np.std(gt) + np.mean(gt)\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93aa92d-5ac5-4e98-b920-41d9bcf9c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "## see the spatial-temporal quality localization for a reference\n",
    "def init_demo_reader(path, i):\n",
    "    from decord import VideoReader, cpu\n",
    "    video_names = [ele.split(',')[0] for ele in open(f'{path}/labels.txt').readlines()]\n",
    "    frame_reader = VideoReader(f'{path}/{video_names[i]}', ctx=cpu(0))\n",
    "    return frame_reader\n",
    "\n",
    "frame_reader = init_demo_reader(dataset_path, q)\n",
    "video_names = [ele.split(',')[0] for ele in open(f'{dataset_path}/labels.txt').readlines()]\n",
    "\n",
    "r_index = random.randrange(len(data['frame_inds']))\n",
    "frame_index = data['frame_inds'][r_index]\n",
    "frame = frame_reader[frame_index]\n",
    "frame_quality_map = demo_result[r_index // 2]\n",
    "fragment = (vfrag.permute(0,2,3,4,1).reshape((128,) + vfrag.shape[-2:] + (3,)).cpu() * inference_set.std + inference_set.mean).numpy()[r_index]\n",
    "\n",
    "frame_quality_map = frame_quality_map.cpu().numpy()\n",
    "qlt = cv2.resize(rescale(frame_quality_map), (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "plt.figure(dpi=450)\n",
    "plt.subplot(221)\n",
    "plt.imshow(frame)\n",
    "plt.subplot(222)\n",
    "plt.imshow(frame / 255. - np.stack((qlt,) + (np.zeros_like(qlt),)*2, -1) / 2.)\n",
    "\n",
    "fqlt = cv2.resize(frame_quality_map, (fragment.shape[1], fragment.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "plt.figure(dpi=300)\n",
    "plt.subplot(221)\n",
    "plt.imshow(fragment / 255.)\n",
    "plt.subplot(222)\n",
    "plt.imshow(fqlt, cmap='gray') #fragment / 255. - np.stack((fqlt,) + (np.zeros_like(fqlt),)*2, -1) / 2.)\n",
    "#plt.savefig(f'demos/demo_{video_names[q].split(\"/\")[-1]}.png')\n",
    "print(frame_quality_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7853b-d2c2-456f-a43d-623f7fb41a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## run inference for a whole testing database\n",
    "## note that the Jupyter program might be relatively slower than running directly with './inference_dataset.py'\n",
    "\n",
    "inference_loader = torch.utils.data.DataLoader(inference_set, batch_size=1, num_workers=4)\n",
    "results = []\n",
    "\n",
    "for i, data in tqdm(enumerate(inference_loader)):\n",
    "    result = dict()\n",
    "    vqfrag = data['video'].to(device).squeeze(0)\n",
    "    with torch.no_grad():\n",
    "        result['pr_labels'] = model(vfrag).cpu().numpy()\n",
    "    result['gt_label'] = data['gt_label'].item()\n",
    "    result['frame_inds'] = data['frame_inds']\n",
    "                                                                                                                                                                                                                                                                                        del data\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57db71-e42b-4cb7-8a9f-22859f34d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating several accuracies indices\n",
    "\n",
    "gt_labels = [r['gt_label'] for r in results]\n",
    "pr_labels = [np.mean(r['pr_labels'][:]) for r in results]\n",
    "opr_labels = pr_labels\n",
    "pr_labels = rescale(pr_labels, gt_labels)\n",
    "\n",
    "srocc = spearmanr(gt_labels, pr_labels)[0]\n",
    "plcc = pearsonr(gt_labels, pr_labels)[0]\n",
    "krocc = kendallr(gt_labels, pr_labels)[0]\n",
    "rmse = np.sqrt(((gt_labels - pr_labels) ** 2).mean())\n",
    "\n",
    "print(f'For dataset {dataset_name} we inference, the accuracy of the model is as follows:\\n  SROCC: {srocc:.4f}\\n  PLCC:  {plcc:.4f}\\n  KROCC: {krocc:.4f}\\n  RMSE:  {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36bf51-0fa1-4bd4-a524-9e327fb39fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ten_133 - np.mean(opr_labels)) / np.std(opr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2878d4-dc68-43ef-be4a-c13443a7bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stableness of Fragments\n",
    "\n",
    "ten_102 = [-0.1089, -0.1156, -0.1259, -0.1196, -0.1227, -0.1156, -0.1100, -0.1113, -0.1181, -0.1126]\n",
    "print('Video No.102', np.mean(ten_102), np.std(ten_102))\n",
    "ten_133 = [-0.0748, -0.0671, -0.0784, -0.0530, -0.0521, -0.0653, -0.0456, -0.0777, -0.0378, -0.0584]\n",
    "print('Video No.133', np.mean(ten_133), np.std(ten_133))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd759b-6400-4d88-b199-f878e985c5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926c587-2cf6-446d-bbd5-b0bd972a9c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_Debug",
   "language": "python",
   "name": "pt1.8v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
