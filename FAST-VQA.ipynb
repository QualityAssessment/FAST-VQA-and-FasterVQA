{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91da64be-faeb-43e6-a08b-61106253d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "from fastvqa.apis import deep_end_to_end_vqa\n",
    "from fastvqa.models import BaseEvaluator, VQABackbone\n",
    "from fastvqa.datasets import VQAInferenceDataset, get_fragments, SampleFrames\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from scipy.stats.stats import kendalltau as kendallr\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "## choose the device you would like to run on\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adee193d-ee6a-4254-a0f9-fd997999fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/hnwu/anaconda3/envs/pt1.8v/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.17445610463619232\n"
     ]
    }
   ],
   "source": [
    "model = deep_end_to_end_vqa()\n",
    "\n",
    "video = torch.randn((3,96,224,224))\n",
    "score = model(video)\n",
    "print(score['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39cd7a5c-a34c-498a-9fc7-f42bd464c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 10 random split experiments,\n",
      "        the mean SROCC is 0.8411 (0.0185), median 0.8421\n",
      "        the mean PLCC  is 0.8531 (0.0175), median 0.8532\n",
      "        the mean KROCC is 0.6530 (0.0199), median 0.6526\n",
      "        the mean RMSE  is 9.0874 (0.6641), median 8.8698\n"
     ]
    }
   ],
   "source": [
    "## Calculate the finetune accuracy on small datasets\n",
    "\n",
    "def get_finetune_results(results):\n",
    "    if 'results' in results:\n",
    "        results = results['results']\n",
    "    srccs = np.array([r[0] for r in results])\n",
    "    plccs = np.array([r[1] for r in results])\n",
    "    krccs = np.array([r[2] for r in results])\n",
    "    rmse = np.array([r[3] for r in results])\n",
    "    ms, ss, mds = np.mean(srccs), np.std(srccs), np.median(srccs)\n",
    "    mp, sp, mdp = np.mean(plccs), np.std(plccs), np.median(plccs)\n",
    "    mk, sk, mdk = np.mean(krccs), np.std(krccs), np.median(krccs)\n",
    "    mr, sr, mdr = np.mean(rmse), np.std(rmse), np.median(rmse)\n",
    "    print(f'''In {len(results)} random split experiments,\n",
    "        the mean SROCC is {ms:.4f} ({ss:.4f}), median {mds:.4f}\n",
    "        the mean PLCC  is {mp:.4f} ({sp:.4f}), median {mdp:.4f}\n",
    "        the mean KROCC is {mk:.4f} ({sk:.4f}), median {mdk:.4f}\n",
    "        the mean RMSE  is {mr:.4f} ({sr:.4f}), median {mdr:.4f}''')\n",
    "\n",
    "finetune_results = torch.load('results/results_finetune_live_vqc_s32*32_ens1.pkl')\n",
    "get_finetune_results(finetune_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57d53ce-9159-4ee5-aa35-0c0439a8c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, False]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pretrained_weights/all_aligned_fragments_32.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26157/1074326157.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'pretrained_weights/all_aligned_fragments_{fsize}.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'state_dict'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt1.8v/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt1.8v/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt1.8v/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pretrained_weights/all_aligned_fragments_32.pth'"
     ]
    }
   ],
   "source": [
    "## defining model and loading checkpoint\n",
    "\n",
    "model = BaseEvaluator().to(device)\n",
    "fsize = 32\n",
    "load_path = f'pretrained_weights/all_aligned_fragments_v0_2.pth'\n",
    "state_dict = torch.load(load_path, map_location='cpu')\n",
    "\n",
    "if 'state_dict' in state_dict:\n",
    "    state_dict = state_dict['state_dict']\n",
    "    from collections import OrderedDict\n",
    "    i_state_dict = OrderedDict()\n",
    "    for key in state_dict.keys():\n",
    "        if 'cls' in key:\n",
    "            tkey = key.replace('cls', 'vqa')\n",
    "            i_state_dict[tkey] = state_dict[key]\n",
    "        else:\n",
    "            i_state_dict[key] = state_dict[key]\n",
    "\n",
    "model.load_state_dict(i_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ad0fe-d8c7-4634-a82e-b4964bf3733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting datasets (if you want to load from existing VQA datasets)\n",
    "\n",
    "dataset_name = 'LIVE_VQC'\n",
    "dataset_path = f'/mnt/lustre/hnwu/datasets/{dataset_name}'\n",
    "\n",
    "inference_set = VQAInferenceDataset(f'{dataset_path}/labels.txt', dataset_path, clip_len=1, aligned=1)\n",
    "                                    #fragments = 224 // fsize, fsize = fsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faf4c9ab-2e99-4e12-8ae1-6f3417ed7607",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Please provide match vclip and align index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11687/2364713081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minference_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/FAST-VQA/fastvqa/datasets/inference_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index, fragments, fsize, tocache)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mvfrag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fragments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mvfrag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fragments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfrags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0mvfrag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvfrag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_fragments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FAST-VQA/fastvqa/datasets/inference_dataset.py\u001b[0m in \u001b[0;36mget_fragments\u001b[0;34m(video, fragments, fsize, aligned, random)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0movideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdur_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mdur_t\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maligned\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Please provide match vclip and align index'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfragments\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragments\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Please provide match vclip and align index"
     ]
    }
   ],
   "source": [
    "inference_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e04e1-7891-449c-8682-f61f72810b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the model with examplar fragment video\n",
    "## with ultra...fast performance\n",
    "\n",
    "## for example from the dataset\n",
    "\n",
    "q = random.randrange(len(inference_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83a188-15e8-40b4-a620-e2b167ed03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inference_set[q]\n",
    "\n",
    "st = time()\n",
    "\n",
    "\n",
    "\n",
    "vfrag = data['video'].to(device)\n",
    "\n",
    "## or, directly get from your input videos as follows\n",
    "## where 'video' is a torch Tensor\n",
    "\n",
    "## from datasets import temporal_sampling (not implemented yet)\n",
    "\n",
    "# data = temporal_sampling(video, 32, 2, 4)\n",
    "\n",
    "# vfrag = get_fragments(data['video']).to(device)\n",
    "\n",
    "demo_result = model(vfrag)\n",
    "print(demo_result.shape)\n",
    "demo_result = demo_result.reshape((-1,) + demo_result.shape[-2:])\n",
    "score = torch.mean(demo_result)\n",
    "end = time()\n",
    "\n",
    "print(f'The quality of the video is {score.item()}, consuming time {end-st:.4f}s.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e742acd-383d-472b-b8a6-307552fc020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(pr, gt=None):\n",
    "    if gt is None:\n",
    "        pr = ((pr - np.mean(pr)) / np.std(pr))\n",
    "    else:\n",
    "        pr = ((pr - np.mean(pr)) / np.std(pr)) * np.std(gt) + np.mean(gt)\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93aa92d-5ac5-4e98-b920-41d9bcf9c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "## see the spatial-temporal quality localization for a reference\n",
    "def init_demo_reader(path, i):\n",
    "    from decord import VideoReader, cpu\n",
    "    video_names = [ele.split(',')[0] for ele in open(f'{path}/labels.txt').readlines()]\n",
    "    frame_reader = VideoReader(f'{path}/{video_names[i]}', ctx=cpu(0))\n",
    "    return frame_reader\n",
    "\n",
    "frame_reader = init_demo_reader(dataset_path, q)\n",
    "video_names = [ele.split(',')[0] for ele in open(f'{dataset_path}/labels.txt').readlines()]\n",
    "\n",
    "r_index = random.randrange(len(data['frame_inds']))\n",
    "frame_index = data['frame_inds'][r_index]\n",
    "frame = frame_reader[frame_index]\n",
    "frame_quality_map = demo_result[r_index // 2]\n",
    "fragment = (vfrag.permute(0,2,3,4,1).reshape((128,) + vfrag.shape[-2:] + (3,)).cpu() * inference_set.std + inference_set.mean).numpy()[r_index]\n",
    "\n",
    "frame_quality_map = frame_quality_map.cpu().numpy()\n",
    "qlt = cv2.resize(rescale(frame_quality_map), (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "plt.figure(dpi=450)\n",
    "plt.subplot(221)\n",
    "plt.imshow(frame)\n",
    "plt.subplot(222)\n",
    "plt.imshow(frame / 255. - np.stack((qlt,) + (np.zeros_like(qlt),)*2, -1) / 2.)\n",
    "\n",
    "fqlt = cv2.resize(frame_quality_map, (fragment.shape[1], fragment.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "plt.figure(dpi=300)\n",
    "plt.subplot(221)\n",
    "plt.imshow(fragment / 255.)\n",
    "plt.subplot(222)\n",
    "plt.imshow(fqlt, cmap='gray') #fragment / 255. - np.stack((fqlt,) + (np.zeros_like(fqlt),)*2, -1) / 2.)\n",
    "#plt.savefig(f'demos/demo_{video_names[q].split(\"/\")[-1]}.png')\n",
    "print(frame_quality_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7853b-d2c2-456f-a43d-623f7fb41a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## run inference for a whole testing database\n",
    "## note that the Jupyter program might be relatively slower than running directly with './inference_dataset.py'\n",
    "\n",
    "inference_loader = torch.utils.data.DataLoader(inference_set, batch_size=1, num_workers=4)\n",
    "results = []\n",
    "\n",
    "for i, data in tqdm(enumerate(inference_loader)):\n",
    "    result = dict()\n",
    "    vqfrag = data['video'].to(device).squeeze(0)\n",
    "    with torch.no_grad():\n",
    "        result['pr_labels'] = model(vfrag).cpu().numpy()\n",
    "    result['gt_label'] = data['gt_label'].item()\n",
    "    result['frame_inds'] = data['frame_inds']\n",
    "                                                                                                                                                                                                                                                                                        del data\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57db71-e42b-4cb7-8a9f-22859f34d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating several accuracies indices\n",
    "\n",
    "gt_labels = [r['gt_label'] for r in results]\n",
    "pr_labels = [np.mean(r['pr_labels'][:]) for r in results]\n",
    "opr_labels = pr_labels\n",
    "pr_labels = rescale(pr_labels, gt_labels)\n",
    "\n",
    "srocc = spearmanr(gt_labels, pr_labels)[0]\n",
    "plcc = pearsonr(gt_labels, pr_labels)[0]\n",
    "krocc = kendallr(gt_labels, pr_labels)[0]\n",
    "rmse = np.sqrt(((gt_labels - pr_labels) ** 2).mean())\n",
    "\n",
    "print(f'For dataset {dataset_name} we inference, the accuracy of the model is as follows:\\n  SROCC: {srocc:.4f}\\n  PLCC:  {plcc:.4f}\\n  KROCC: {krocc:.4f}\\n  RMSE:  {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36bf51-0fa1-4bd4-a524-9e327fb39fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ten_133 - np.mean(opr_labels)) / np.std(opr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2878d4-dc68-43ef-be4a-c13443a7bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stableness of Fragments\n",
    "\n",
    "ten_102 = [-0.1089, -0.1156, -0.1259, -0.1196, -0.1227, -0.1156, -0.1100, -0.1113, -0.1181, -0.1126]\n",
    "print('Video No.102', np.mean(ten_102), np.std(ten_102))\n",
    "ten_133 = [-0.0748, -0.0671, -0.0784, -0.0530, -0.0521, -0.0653, -0.0456, -0.0777, -0.0378, -0.0584]\n",
    "print('Video No.133', np.mean(ten_133), np.std(ten_133))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd759b-6400-4d88-b199-f878e985c5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926c587-2cf6-446d-bbd5-b0bd972a9c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_Debug",
   "language": "python",
   "name": "pt1.8v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
