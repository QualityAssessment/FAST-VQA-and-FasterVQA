# FAST-VQA

The official open source training and inference code for future paper 'FAST-VQA: Efficient End-to-end Video Quality Assessment with Fragment Sub-sampling'.

Link on Preprint Edition of Paper:

## Results

We reach best performance on LSVQ-1080p test set while with much reduced FLOPs.

![GFLOPs-performance](./demos/GFLOPs-performance.png)

While the proposed Fragment Subsampling can keep both local quality information and the contextual information, the following FANet can extract them through end-to-end deep learning, as the following figure shows.

![GFLOPs-performance](./demos/GFLOPs-performance.png)

See in [demos](./demos/) for examples.


## Build FAST-VQA

### Requirements

The original method is build with

- python=3.8.8
- torch=1.8.1
- torchvision=0.9.1

while using decord module to read original videos (so that you don't need to make any transform on your original .mp4 input).

To get all the requirements, please run

```shell
pip install -r requirements.txt
```

Or directly run 

```shell
pip install .
```

to build the full FAST-VQA.

## Test FAST-VQA

### Inference on Scripts

You can install this directory by running

```shell
pip install .
```

And download the pretrained weights from [GDrive](pretrained_weights/README.md) and put them into `/pretrained_weights`.

Then you can embed these lines into your python scripts:

```python
from fastvqa import deep_end_to_end_vqa

video = torch.randn((3,96,224,224))
vq_evaluator = deep_end_to_end_vqa(pretrained=True, pretrained_path='pretrained_weights/fast_vqa_v0_3.pth')
score = vq_evaluator(video)
print(score)
```

### Benchmarking FAST-VQA

You can directly benchmark the model with mainstream benchmark VQA datasets.

```shell
python inference.py -d $DATASET$
```

Available datasets are LIVE_VQC, KoNViD, (experimental: CVD2014, YouTubeUGC), LSVQ (or 'all' if you want to infer all of them).



## Train FAST-VQA


### Train from scratch

You might need to download the original Swin-T weights from the following link to initialize the model.

[Swin-T Weights](https://github.com/SwinTransformer/storage/releases/download/v1.0.4/swin_tiny_patch244_window877_kinetics400_1k.pth)

#### Intra Dataset Training

This training will split the dataset into 10 random train/test splits (with random seed 42) and report the best result on the random split of the test dataset. 

```shell
python inference.py -d $DATASET$ --from_ar
```

Supported datasets are KoNViD-1k, LIVE_VQC, CVD2014, YouTube-UGC.

#### Cross Dataset Training

This training will do no split and directly report the best result on the provided validation dataset.

```shell
python inference.py -d $TRAINSET$-$VALSET$ --from_ar -lep 0 -ep 30
```

Supported TRAINSET is LSVQ, and VALSETS can be LSVQ(LSVQ-test+LSVQ-1080p), KoNViD, LIVE_VQC.


### Finetune with provided weights

#### Intra Dataset Training

This training will split the dataset into 10 random train/test splits (with random seed 42) and report the best result on the random split of the test dataset. 

```shell
python inference.py -d $DATASET$ --from_ar
```

Supported datasets are KoNViD-1k, LIVE_VQC, CVD2014, YouTube-UGC.

## Using FASTER-VQA

You can add the argument `-m FASTER` in any scripts above to switch to FASTER-VQA instead of FAST-VQA.

## Abstract

Video quality assessment (VQA) on high resolution videos is limited by high computational cost: existing methods often require tens of seconds for inference on them and unable to be trained directly. Therefore, an efficient sub-sampling approach that preserves quality-related information on these videos will be significantly helpful. Based on the observation that randomly sampled mini-patches of videos can effectively represent the video's regional quality around them, we design the fragment sub-sampling strategy, including grid mini-patches sub-sampling (GMS) and temporal fragment alignment (TFA) constraint to sub-sample videos in diverse resolutions. We further build the fragment attention network (FANet), an end-to-end transformer model with gated relative position bias (GRPB) to learn self-attention both within and across fragments. With fragment sub-sampling and FANet, the proposed **Fragment Sub-sample Transformer for VQA (FAST-VQA)** enables efficient and effective end-to-end deep video quality assessment (VQA) regardless of its original resolution. With **97.6%** reduced FLOPs, the proposed method has outperformed state-of-the-art methods by **10%** on high-resolution videos. FAST-VQA also provides effective VQA-oriented deep features which transfers well on smaller VQA datasets. We have also done complete ablation study to prove the effectiveness of both fragment sub-sampling and FANet, and also provide analysis on the prediction stability of FAST-VQA and local quality maps generated by it. We believe the FAST-VQA will bring deep VQA methods into practical use with its prediction effectiveness and significantly improved efficiency.

